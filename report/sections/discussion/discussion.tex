
\chapter{Discussion}
\label{discussion}
This section consists of an overall discussion, an analysis of vital parts from the results and methology that were used in this study is presented. A short analysis of the interview answers and the correlation between these are presented. The results from testing and evaluation of prototype fidelities are also discussed, along with possible changes and features based on behaviour and ideas from participants during the testing sessions.

The tools presented in this thesis have potential to make significant improvements to the experience design process for VR project. By continuous iteration of the current prototype along with changes suggested in this section, this tool could see commercial using in a variety of fields. The simplicity in the design makes it avaliable to more then just programmers and designers, as the evaluation showed no significant difference adoption of this tool between subjects with and without programming experience, it proves to be a plausible tool for communicating ideas and concepts in cross-functional teams. The hardships during the design and development of these tools is in itself a verification of the problem at hand, and the current need for a tool like the one presented. It is however hard to know where the industry will be in the foreseeable future and which role VR will have in that future. With evolved software and hardware could improve the precision and portability of the VR industry that could once again change the field of VR as we know it today.

\section{Interviews}
After the interviews it became clear that two different solutions should be implemented, in best case. One of the biggest advantages of having two different tools is that a a tool for a low-level medium (created for a less powerful device) this tool could be used by several people in parallel with the much more convenient and portable Google Daydream\footnote{https://vr.google.com/daydream/}.Unfortunately this was tweaked however as the VR editors that exists only support hi-fidelity HMD's (HTC Vive\footnote{https://www.vive.com/eu/} and Oculus Rift\footnote{https://www.oculus.com/rift/}) and would therefore require a different implementation approach.

\section{Lo-fi Prototype}
Creating a lo-fi prototype for VR interfaces and VR concepts required another approach then for regular screen applications. A significant part of this phase was dedicated to finding an appropriate way of creating, validating and testing. Using TiltBrush to visualize a hand-sketched concept was a great technique for getting relative sizing and perspective to scenarios containing these concepts. The use of real-world objects and a real HMD controller as part of the test suite worked extremely well when evaluating concept sketches with a minimal overhead. By using post-its on objects, their current state were displayed in a fast and easy way.

The initial user tests of the lo-fi concepts where perhaps the most game-changing in terms of major changes to the prototype. Since the tests consisted of the users trying to interact with the tool and the environment, they are not informed on what kinds of interactions that are available. This gives a better view om the logic of what the user expects from the environment. For example: The initial concepts that involved movements on the trackpad of the controller for selection within a pie-menu (figure \ref{fig:relatedwork:piemenu}). With knowledge of the interaction patterns, the concept seemed really thought-out and logical, as well as it included ease of use. When testing these features however, none of the users actually tried to use this technique without hints from the test leader. Instead they tried to point to items on the menu with the ray-cast (Section \ref{theory:toolsandtech:raycast}) until they were told that nothing happens. This result strengthened the idea of pointing instead of using the touch pad, and when users were asked why the came to that conclusion, the answer where because they pointed and clicked on the menu to open it. At the time of testing there were no cues to actually introduce the possible interaction with the touchpad and because of that, this approach was still carried into the hi-fi prototype as a concept as well.

Another insight unexpectedly appeared during the user testing of the lo-fi prototypes was the importance of the scene hierarchy. In this, all objects of the scene are listed as subobjects of each other. Initially the design did not include this, as to eliminate all complex features that could distract the user from their primary objective, to create. Without this however, the sequence of creating a new object gave the user no relation between the new object and the rest of the environment. The usage of the hierarchy also enables users to interact with and manipulate occluded objects.


\section{Hi-fi Prototype}
Creating the hifi prototype was in itself a demostration of the need for tools like the one explained in this study. The transitions between Illustrator, Sketch and Unity was pretty smooth, but time-consuming. The majority of hours spent on this testing suite were creating the scripts and logic for all features and functionality of the concept. Using Wizard of Oz to minimize the amount of time on feature implementation was a great technique for this medium and methology, as the user when immerged is unaware of the wizards' actions. The advantages of this was however not recognized until the pre-test run. In retrospect the more features in the test suite could have been designed to handle actions from the test leader (wizard) instead of a full implementation.

The results from the usertest of the hi-fi prototype surfaces issues and behaviours based on the current design and concept, some of them connected to the dimensions that were missing in the lo-fi testing. This displays the validity of hi-fi testing for VR concept designs as it recovers issues that would otherwise remain hidden, and correlates with the reasoning in section \ref{method:prototype:hifi}. After conducting tests on the lo-fi prototype, the selector interface was prioritized higher than a controller-based interface for access to object manipulation tools. This was due to its direct connection with the object and that it was easier to understand when testing in the real world. When the hi-fi prototype where developed and initially tested, this changed again). When the user selects an object that is far away, the selector next to it appeared so small that it became very hard to read and interact with it. Perceptive scaling were added to the interface to avoid this, which caused the interface to occlude other objects adjacent to the selected object (more about occlusion in section \ref{theory:interactionissues:occlusion}). These problems shed some new light on the previously ignored solution, a controller-based interface using the touchpad. The interactions with this interface were chosen because they can be achieved using one controller , which supports both middle- and high-level devices (more information about devices in section \ref{theory:HMD}), and its accessibility. The apparent issue with this approach that was discovered in lo-fi tests were addressed by changing the color of the selected object when the menu is activated. The idea is to connect the touchpad interactions with a specific color, that separates it from the primary interactions done with the raycast.

The controller-based UI (piemenu) was a good solution for object manipulation, as users adopted the interaction style quickly and commented on the practicality of the concept. However there were issues concerning the interaction patterns with this UI. As the action of each menu item is triggered upon selecting, the subjects had a hard time focusing on both the selected object and the menu itself in order to get feedback on the action. One solution for this is a mode-based interface; Where an action triggered with the selection button and the triggered action is based on the current selection in the piemenu. This approach would also solve the practial complications with object selection that appeared, even though the selection technique worked well in general. The issues involve a user selecting an object and it getting automatically grabbed, which lets the raycast ignore the surface of the object. In practice the object will instantly move to the surface behind it, as that is what the raycast hits. By using a mode-based UI from the piemenu, the object would first have to be selected, the grabbed in order to move it. Another way of of solving the focus issue is by displaying a mirrored graphic of the piemenu on the object itself (similar to the selector that was presented in \ref{result:prototype:lofi} but without raycast interaction)

In the prototype that was being tested, the objects were not highlighted upon selection. The only indication was the apearance of the reference plane. Adding a bounding box to the selected object when selected or moved is preferred. A similar issue appear when the reference plane is grabbed with intention of moving it, as there is no visual feedback that the plane is grabbed. This caused the users to re-grab the plane multiple times in confusion. The confusion was increased when the plane did not move based on rotational changes of the controller. Several times during the testing sessions there were scenarios where the user would try to elevate an object from the floor (with a horizontal reference plane) by increasing the rotation of the controller slightly, which still leaves the raycast hit on the floor. Because the reference plane will position itself according to the \underline{surface} that the raycast hits and not the difference from the original rotation of the controller, the object will not move. This was mainly the plane had no desired surface as a reference and an issue that the majority of participants encountered, when discussed some enhancements to this element were discovered:
\begin{itemize}
  \item Introducing two \textbf{supporting planes} on the remaining axis' allows the user to manualy position the plane by tracking the hitpoint on these planes. This feature would come with an option to toggle the visibility of supporting planes. depending on needs of the user.
  \item Tracking change in \textbf{spatial position of controller}. Several instances of this behaviour were found when a test-subject tried to change the elevation of a box on the ground by lifting on of the controllers higher into the air. This proves to be the go-to interaction for first time users but does come with practical issues, one of those being that the movement is limited to the physical reach of the user.
  \item Introducing an \textbf{axis arrow}. As test subject initially thought that changes in rotation of the controller is relative to the position of the reference plane, introducing that as a possibility could add to the practicality of moving an plane to a "floating" position.
\end{itemize}
Trying to place objects next to eachother was also a recurring issue during the test sessions, as the test subjects wanted to place an object pixel-to-pixel with another object using the raycast. This was difficult at longer distances as the raycast hit would oscillate over the targeted point, which could send the object flying between different surfaces. What was requested some users is adding snap-to functionality when pointing close to an edge, that would allow edges to work as magnets, sticking to eachother when they get close to eachother.

The response from being in a seated position while working in VR was very positive. Several test subjects explained that they never really thought about the fact that they were seated or that it limited them in any way. They also utilized this to improve accuracy of the raycast selection, by resting arms or controller on their legs or armrest.
Using teleportation as a transportation technique also works well from a seated postition but could be complimented with zoom or scaling of the world. Scaling the world around the user could benefit the process of moving object in larger environments and handling larger objects. A zoom feature would keep the user in the same position and only increasing the scale of their FOW, which could help users to make small changes on small or far away objects. The main UI (BeltUI) was also commented as a good fit for the seated position, along with some remarks about improvements to the interaction technique. Some test subjects found the raycast technique to be awkward, forcing them into less ergonomic body positions. Based on this, some improvements that surfaced were to use the trackpad to navigate between objects or have the controllers act as drumsticks and hitting the UI buttons like drums.

Apart from the features that were presented in this section, there were a few that would be a good addition to the current concept;

\begin{itemize}
  \item Animating an object being deleted och duplicated through the raycast ray. This would act as additional visual feedback for the user.
  \item Drawing an outline around several objects with the raycast for multiple selection. Objects that are inside the cone that is created between the controller and the marked area will be selected. A reference plane will be joined to the selected area in order to resize depth.
\end{itemize}
